# TwitterAnalist-IntroducaoAProgramcao

Trabalho **em andamento** da disciplina de Introdução a Programação do curso de ADS do IFPE Paulista 

O Twitter Analyst é um programa de coleta, análise e visualização de dados textuais do Twitter que, ao utilizar-se de técnicas como web scraping, análise de dados e aprendizado de máquina, pretende oferecer ao usuário interessado um diagnóstico modesto, porém decente, da opinião pública ou da percepção específica de alguns perfis virtuais, analisando/classificando posicionamentos e sentimentos automaticamente a partir de dados textuais coletados.

Funcionalidades: 

O programa que se pretende implementar é um software de mineração de dados textuais (coleta, análise e visualização de tweets). Este deverá dar suporte às funcionalidades descritas a seguir: 

1. Registrar os atributos da consulta que o usuário deseja realizar, como o tipo (por perfil ou por tópico) ou o tema da busca. Isso será realizado através da apuração de palavras-chave e hashtags relacionadas, perfis de interesse, quantidade de tweets a serem analisados, entre outras features dos tweets/perfis, como localização, data, quantidade de seguidores, retweets e curtidas. 
2. Através das informações fornecidas acima pelo usuário na linha de comando, o programa inicia sua consulta, realizando uma coleta de dados no Twitter. Pretende-se fazer tal coleta através de bibliotecas como Tweepy (acesso aos dados via API padrão do Twitter, que limita a quantidade de dados coletados), GetOldTweets3 (menos funcionalidades, porém acesso mais amplo aos tweets) ou Twarc (gerencia as taxas de limite da API e possibilita coletar tweets antigos a partir de seu ID).
3. O programa, então, deve estruturar os dados coletados (retornados no formato json) através de estruturas embutidas de python como listas e dicionários, mas também com o auxílio de estruturas de dados que facilitam o pré processamento e a análise, como arrays e data frames, através de bibliotecas como numpy e pandas.  
4. No entanto, antes de transformar os dados no dataframe que vai ser processos pelos algoritmos de Machine Learning(ML), o programa vai executar um pipeline de pré processamento automático dos dados coletados, com técnicas de tokenização, lowercasing e remoção de stop words/pontuações, além de coletar hashtags e menções. Nessa etapa, o usuário que conhece as técnicas vai poder escolher manualmente quais prefere utilizar, já que a implementação destas varia de acordo com a aplicação de modelos ML.  Mesmo assim, o programa oferecerá um pipeline padrão para o usuário leigo em limpeza e normalização de dados textuais. Aqui será utilizada a biblioteca NLTK para NLP em python, que possui módulos que auxiliam nessa etapa de pré processamento.
5. Tendo em vista que o programa realiza a coleta e análise automaticamente e em tempo real, os dados coletados não serão utilizados para treinar um algoritmo de ML, mas para realizar a classificação através de modelos já treinados anteriormente através de aplicações do estado da arte de análise de sentimento e detecção de posicionamentos (stance detection). Além da análise preditiva oferecida pelos modelos de aprendizado, o programa também propõe oferecer estatísticas descritivas de acordo com o tema e features de consulta requisitadas. 
6. Para concluir, o programa deve oferecer uma possibilidade de visualização dessas estatísticas em forma de gráficos e tabelas, através da biblioteca matplotlib, para que o usuário possa explorar os resultados visualmente.
